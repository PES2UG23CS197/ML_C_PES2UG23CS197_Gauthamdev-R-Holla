{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5x6kM60usj-"
      },
      "source": [
        "# **ML Lab Week 10: SVM Classifier Lab**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEsNQuo-u78U"
      },
      "source": [
        "**Objective:** The goal of this lab is to understand and implement Support Vector Machine (SVM) classifiers. You will train SVMs using three different kernels: **Linear, Radial Basis Function (RBF), and Polynomial**, on distinct datasets. You will then evaluate their performance using standard classification metrics and visualize their decision boundaries to see how they separate data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bN3UziZdvQTX"
      },
      "source": [
        "## **Core Concepts**\n",
        "**Support Vector Machine (SVM):** A powerful supervised learning algorithm that finds an optimal hyperplane to separate data points of different classes.\n",
        "\n",
        "**Kernel Trick:** A technique that allows SVMs to solve non-linear problems by transforming data into a higher-dimensional space.\n",
        "\n",
        "- Linear Kernel: Creates a straight-line decision boundary.\n",
        "\n",
        "- RBF Kernel: Creates a complex, non-linear boundary, like a circle or a wave.\n",
        "\n",
        "- Polynomial Kernel: Creates a curved, polynomial decision boundary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOhbbi9Gvv-m"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXTI6rtVvhyV"
      },
      "source": [
        "## 1. Setup and Imports\n",
        "First, let's import the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWNhNgVourlx"
      },
      "outputs": [],
      "source": [
        "# Core libraries for data manipulation and analysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Libraries for machine learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Libraries for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set a style for all plots\n",
        "sns.set_style(\"whitegrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m7amGeG4ylk"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX7UWAvBvoMr"
      },
      "source": [
        "## 2. Helper Function for Visualization\n",
        "This helper function plots the decision boundaries for our trained models. It will work for all our datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmslP908vsy2"
      },
      "outputs": [],
      "source": [
        "def plot_decision_boundaries(X, y, model, title):\n",
        "    \"\"\"\n",
        "    Visualizes the decision boundaries of a trained classifier.\n",
        "    (Corrected version)\n",
        "    \"\"\"\n",
        "    # Create a meshgrid to plot the decision boundary\n",
        "    h = .02  # step size in the mesh\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "    # Predict the class for each point in the meshgrid\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plot the decision boundary and the data points\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "\n",
        "    # Plot the training points\n",
        "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.title(title)\n",
        "\n",
        "    # Get unique labels and ensure they are a list for the legend function\n",
        "    unique_labels = np.unique(y)\n",
        "    if len(unique_labels) == 2:\n",
        "        legend_labels = ['Class 0', 'Class 1']\n",
        "    else:\n",
        "        legend_labels = list(unique_labels.astype(str)) # Convert numpy array to a list\n",
        "\n",
        "    plt.legend(handles=scatter.legend_elements()[0], labels=legend_labels)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ2zxb4z409o"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym9ird8z48yY"
      },
      "source": [
        "## 3. Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR_X4RilGxLa"
      },
      "source": [
        "# **PART 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op6vliWcv0ri"
      },
      "source": [
        "## Dataset 1: The Moons Dataset\n",
        "The Moons dataset is a synthetic dataset designed to test non-linear classification algorithms. The data points are shaped like two interlocking half-moons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj4mhhBMv8iN"
      },
      "source": [
        "### Step 1.1: Generate and Prepare the Data\n",
        "We will generate the data using scikit-learn and apply feature scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-gQk0Phv_nD"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_moons\n",
        "\n",
        "# Generate non-linearly separable data\n",
        "X_moons, y_moons = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train_moons, X_test_moons, y_train_moons, y_test_moons = train_test_split(\n",
        "    X_moons, y_moons, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Scale the features\n",
        "# Feature scaling is crucial for SVMs to perform optimally.\n",
        "scaler_moons = StandardScaler()\n",
        "X_train_moons_scaled = scaler_moons.fit_transform(X_train_moons)\n",
        "X_test_moons_scaled = scaler_moons.transform(X_test_moons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJgrWHhYPv6Y"
      },
      "outputs": [],
      "source": [
        "# Visualize the Moons dataset\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Moons Dataset')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlhDtumxwCma"
      },
      "source": [
        "### Step 1.2: Train and Evaluate SVM Kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DqUfmV2wHxo"
      },
      "outputs": [],
      "source": [
        "kernels = ['linear', 'rbf', 'poly']\n",
        "models_moons = {}\n",
        "\n",
        "for kernel in kernels:\n",
        "    # TODO: Initialize SVM with the current kernel\n",
        "    svm_model = None # Replace with SVC(...) initialization\n",
        "\n",
        "    # TODO: Train the model\n",
        "    # svm_model.fit(...)\n",
        "\n",
        "    # Store the trained model\n",
        "    models_moons[kernel] = svm_model\n",
        "\n",
        "    # TODO: Make predictions\n",
        "    # y_pred_moons = svm_model.predict(...)\n",
        "\n",
        "    # TODO: Replace with your SRN\n",
        "    print(f\"SVM with {kernel.upper()} Kernel <PES1UG23CSXXX>\")\n",
        "    print(classification_report(y_test_moons, y_pred_moons))\n",
        "    print(\"-\" * 40 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvmZxuNDwKBl"
      },
      "source": [
        "### Step 1.3: Visualize Decision Boundaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKZcANeVwHxp"
      },
      "outputs": [],
      "source": [
        "#TODO: Replace with your SRN\n",
        "for kernel, model in models_moons.items():\n",
        "    plot_decision_boundaries(\n",
        "        X_train_moons_scaled,\n",
        "        y_train_moons,\n",
        "        model,\n",
        "        title=f'Moons Dataset - SVM with {kernel.upper()} Kernel <PES1UG23CSXXX>'\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKAFMqqDwQ9T"
      },
      "source": [
        "### Analysis Questions for Moons:\n",
        "\n",
        "1. Based on the metrics and the visualizations, what inferences about the performance of the Linear Kernel can you draw?\n",
        "\n",
        "2. Compare the decision boundaries of the RBF and Polynomial kernels. Which one seems to capture the shape of the data more naturally?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHJJLLShG70H"
      },
      "source": [
        "# **PART 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMfpHUHfwbng"
      },
      "source": [
        "## Dataset 2: Banknote Authentication\n",
        "This is a real-world binary classification dataset where the goal is to predict whether a banknote is genuine or forged based on features extracted from a digital image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsWjXk8Fwhtk"
      },
      "source": [
        "### Step 2.1: Load and Prepare the Data\n",
        "We will load this data from a public URL using pandas. We will use the variance and skewness of the image transform as our features for visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szxCkgbZwpe7"
      },
      "outputs": [],
      "source": [
        "# Load the dataset from a URL\n",
        "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt'\n",
        "banknote_df = pd.read_csv(url, header=None, names=['variance', 'skewness', 'curtosis', 'entropy', 'class'])\n",
        "\n",
        "# Select features and target\n",
        "X_banknote = banknote_df[['variance', 'skewness']].values\n",
        "y_banknote = banknote_df['class'].values\n",
        "\n",
        "# Split data\n",
        "X_train_banknote, X_test_banknote, y_train_banknote, y_test_banknote = train_test_split(\n",
        "    X_banknote, y_banknote, test_size=0.3, random_state=42, stratify=y_banknote\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler_banknote = StandardScaler()\n",
        "X_train_banknote_scaled = scaler_banknote.fit_transform(X_train_banknote)\n",
        "X_test_banknote_scaled = scaler_banknote.transform(X_test_banknote)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvcGpcIpPzlV"
      },
      "outputs": [],
      "source": [
        "# Visualize the Banknote Authentication dataset\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_banknote[:, 0], X_banknote[:, 1], c=y_banknote, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "plt.xlabel('Variance')\n",
        "plt.ylabel('Skewness')\n",
        "plt.title('Banknote Authentication Dataset')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8CPFt9vwubY"
      },
      "source": [
        "### Step 2.2: Train and Evaluate SVM Kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGq0uoiXwHxr"
      },
      "outputs": [],
      "source": [
        "models_banknote = {}\n",
        "\n",
        "for kernel in kernels:\n",
        "    # TODO: Initialize and train the SVM\n",
        "    svm_model = None # Replace with SVC(...) initialization\n",
        "    # svm_model.fit(...)\n",
        "\n",
        "    # Store the model\n",
        "    models_banknote[kernel] = svm_model\n",
        "\n",
        "    # TODO: Make predictions\n",
        "    # y_pred_banknote = svm_model.predict(...)\n",
        "\n",
        "    # TODO: Replace with your SRN\n",
        "    print(f\"SVM with {kernel.upper()} Kernel <PES1UG23CSXXX>\")\n",
        "    print(classification_report(y_test_banknote, y_pred_banknote, target_names=['Forged', 'Genuine']))\n",
        "    print(\"-\" * 40 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jKxGYTWw2QX"
      },
      "source": [
        "### Step 2.3: Visualize Decision Boundaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s56S5YrEwHxs"
      },
      "outputs": [],
      "source": [
        "#TODO: Replace with your SRN\n",
        "for kernel, model in models_banknote.items():\n",
        "    plot_decision_boundaries(\n",
        "        X_train_banknote_scaled,\n",
        "        y_train_banknote,\n",
        "        model,\n",
        "        title=f'Banknote Dataset - SVM with {kernel.upper()} Kernel <PES1UG23CSXXX>'\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1rCis5Yw-lT"
      },
      "source": [
        "### Analysis Questions for Banknote:\n",
        "1. In this case, which kernel appears to be the most effective?\n",
        "2. The Polynomial kernel shows lower performance here compared to the Moons dataset. What might be the reason for this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHD2_NTgObyb"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj525Tt6G_xb"
      },
      "source": [
        "# **PART 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj4yO-6D4bKU"
      },
      "source": [
        "## 4. Understanding the Hard and Soft Margins\n",
        "Soft Margin (Small C value, e.g., 0.1): A smaller C value creates a \"softer\" margin, meaning the model is more tolerant of misclassifications. This results in a wider margin and can lead to better generalization, especially with noisy data.\n",
        "\n",
        "Hard Margin (Large C value, e.g., 100): A larger C value creates a \"harder\" margin. The model will try to classify every data point correctly, resulting in a narrower margin. This can lead to overfitting if the data has outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4q4ruzW59OE"
      },
      "source": [
        "Let's create a dataset that is mostly linearly separable but has some noise, which is perfect for understanding the difference between hard and soft margins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoLZlZ4F6G_z"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "# Generate linearly separable data with some noise\n",
        "X_linear, y_linear = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=0.60)\n",
        "\n",
        "# Add some outliers\n",
        "outliers_X = np.array([[0.5, 2.5], [1.5, 0.5]])\n",
        "outliers_y = np.array([1, 0])\n",
        "X_linear = np.concatenate([X_linear, outliers_X])\n",
        "y_linear = np.concatenate([y_linear, outliers_y])\n",
        "\n",
        "\n",
        "# Split and scale the data\n",
        "X_train_linear, X_test_linear, y_train_linear, y_test_linear = train_test_split(\n",
        "    X_linear, y_linear, test_size=0.3, random_state=42\n",
        ")\n",
        "scaler_linear = StandardScaler()\n",
        "X_train_linear_scaled = scaler_linear.fit_transform(X_train_linear)\n",
        "X_test_linear_scaled = scaler_linear.transform(X_test_linear)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTrTM0I56MH3"
      },
      "source": [
        "Now, let's train two SVM models with a linear kernel: one with a small C (soft margin) and one with a large C (hard margin)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsqmGxX66M7Q"
      },
      "outputs": [],
      "source": [
        "# Soft Margin SVM (small C)\n",
        "# TODO: Create a linear SVM model with a small C value (e.g., 0.1) for a soft margin.\n",
        "#          - Set the kernel to 'linear'.\n",
        "#          - Set C to 0.1.\n",
        "#          - Set random_state to 42 for consistent results.\n",
        "svm_soft = None\n",
        "\n",
        "# TODO: Fit the soft margin model to the training data (X_train_linear_scaled, y_train_linear).\n",
        "svm_soft.fit(...)\n",
        "\n",
        "# TODO: Replace with your SRN\n",
        "plot_decision_boundaries(X_train_linear_scaled, y_train_linear, svm_soft, title='Soft Margin SVM (C=0.1) <PES1UG23CSXXX>')\n",
        "\n",
        "# Hard Margin SVM (large C)\n",
        "# TODO: Create a linear SVM model with a large C value (e.g., 100) for a hard margin.\n",
        "#          - Set the kernel to 'linear'.\n",
        "#          - Set C to 100.\n",
        "#          - Set random_state to 42.\n",
        "svm_hard = None\n",
        "\n",
        "# TODO: Fit the hard margin model to the training data\n",
        "svm_hard.fit(...)\n",
        "\n",
        "# TODO: Replace with your SRN\n",
        "plot_decision_boundaries(X_train_linear_scaled, y_train_linear, svm_hard, title='Hard Margin SVM (C=100) <PES1UG23CSXXX>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uuF7J9_8I2I"
      },
      "source": [
        "### Analysis Questions\n",
        "\n",
        "1. Compare the two plots. Which model, the \"Soft Margin\" (C=0.1) or the \"Hard Margin\" (C=100), produces a wider margin?\n",
        "\n",
        "2. Look closely at the \"Soft Margin\" (C=0.1) plot. You'll notice some points are either inside the margin or on the wrong side of the decision boundary. Why does the SVM allow these \"mistakes\"? What is the primary goal of this model?\n",
        "\n",
        "3. Which of these two models do you think is more likely to be overfitting to the training data? Explain your reasoning.\n",
        "\n",
        "4. Imagine you receive a new, unseen data point. Which model do you trust more to classify it correctly? Why? In a real-world scenario where data is often noisy, which value of C (low or high) would you generally prefer to start with?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMb7P0Oo6O2N"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slAWozHbxgsJ"
      },
      "source": [
        "## 5. Lab Summary and Conclusion\n",
        "In this lab, you have:\n",
        "\n",
        "- Trained SVM classifiers on three distinct datasets: one synthetic non-linear, one real-world binary, and one high-dimensional multi-class.\n",
        "\n",
        "- Implemented and compared three common kernels: Linear, RBF, and Polynomial.\n",
        "\n",
        "- Evaluated model performance using standard classification reports.\n",
        "\n",
        "- Visualized decision boundaries to understand how each kernel operates on different data distributions.\n",
        "\n",
        "- Understood hard and soft margins."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlkKDHg8FRku"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
