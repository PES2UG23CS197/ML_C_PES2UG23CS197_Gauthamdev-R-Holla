{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bixlHbSNBjix",
        "AVHIiaD7Bhna"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Part A**\n",
        "Count / Frequency based Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "bixlHbSNBjix"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMQMMdXB75_2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    f1_score\n",
        ")\n",
        "\n",
        "# =======================================================\n",
        "# TODO: Students must implement the following steps:\n",
        "# 1. Complete the fit method in NaiveBayesClassifier (4 TODOs for log prior and log likelihood calculation).\n",
        "# 2. Complete the predict method in NaiveBayesClassifier (2 TODOs for log probability accumulation and final argmax).\n",
        "# 3. Complete the data loading calls in Section 2.\n",
        "# 4. Initialize CountVectorizer with proper parameters in Section 3a.\n",
        "# 5. Complete the feature transformation (fit_transform and transform) in Section 3a.\n",
        "# 6. Initialize and fit the custom nb_model in Section 3b.\n",
        "# 7. Use the fitted nb_model to generate predictions in Section 4.\n",
        "# =======================================================\n",
        "\n",
        "\n",
        "# Data loading function (DO NOT CHANGE)\n",
        "def load_pubmed_rct_file(filepath):\n",
        "    \"\"\"\n",
        "    Reads a .txt file from the PubMed 20k RCT dataset.\n",
        "    Returns a DataFrame with 'label' and 'sentence'.\n",
        "    \"\"\"\n",
        "    labels, sentences = [], []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line or '\\t' not in line:\n",
        "                continue\n",
        "            label, sent = line.split('\\t', maxsplit=1)\n",
        "            labels.append(label)\n",
        "            sentences.append(sent)\n",
        "    return pd.DataFrame({'label': labels, 'sentence': sentences})\n",
        "\n",
        "\n",
        "# Implementing Multinomial Naive Bayes from scratch\n",
        "class NaiveBayesClassifier:\n",
        "    \"\"\"\n",
        "    Multinomial Naive Bayes Classifier implemented from scratch.\n",
        "    It is suitable for both Count and TF-IDF features.\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha\n",
        "        self.class_priors = {}\n",
        "        self.feature_log_probs = {}\n",
        "        self.classes = None\n",
        "        self.vocabulary_size = 0\n",
        "\n",
        "    def fit(self, X_counts, y):\n",
        "        y_array = y.to_numpy()\n",
        "        self.classes = np.unique(y_array)\n",
        "        self.vocabulary_size = X_counts.shape[1]\n",
        "\n",
        "        for c in self.classes:\n",
        "            X_c = X_counts[y_array == c]\n",
        "\n",
        "            # // TODO: Calculate the log prior and store it in self.class_priors[c]\n",
        "            # Calculate Class Prior P(C): log(P(C))\n",
        "            # P(C) = (Number of samples in class c) / (Total number of samples)\n",
        "            self.class_priors[c] = None\n",
        "\n",
        "            feature_sum = X_c.sum(axis=0).A1\n",
        "            total_mass = np.sum(feature_sum)\n",
        "\n",
        "            # Apply Laplace smoothing (additive smoothing, alpha=1.0 default):\n",
        "            # P(w_i | C) = (count(w_i, C) + alpha) / (total_words_in_C + alpha * vocab_size)\n",
        "\n",
        "            # // TODO: Calculate the numerator (with Laplace smoothing)\n",
        "            numerator = None\n",
        "\n",
        "            # // TODO: Calculate the denominator (with Laplace smoothing)\n",
        "            denominator = None\n",
        "\n",
        "            # // TODO: Calculate the log likelihood (log(numerator / denominator))\n",
        "            self.feature_log_probs[c] = None\n",
        "\n",
        "    def predict(self, X_counts):\n",
        "        y_pred = []\n",
        "        for i in range(X_counts.shape[0]):\n",
        "            scores = {}\n",
        "\n",
        "            x_i = X_counts.getrow(i)\n",
        "\n",
        "            for c in self.classes:\n",
        "                log_prob = self.class_priors[c]\n",
        "                log_likelihoods = self.feature_log_probs[c]\n",
        "\n",
        "                non_zero_indices = x_i.indices\n",
        "                non_zero_data = x_i.data\n",
        "\n",
        "                # // TODO: Complete the log probability calculation for the likelihood term\n",
        "                # Add log likelihoods contribution (Log-Sum Trick):\n",
        "                # log_prob += sum(count(w_i) * log(P(w_i|C)))\n",
        "\n",
        "                log_prob += 0\n",
        "                scores[c] = log_prob\n",
        "\n",
        "            # // TODO: Find the key (class label) with the maximum score\n",
        "            predicted_class = None\n",
        "\n",
        "            y_pred.append(predicted_class)\n",
        "\n",
        "        # // TODO: Return the final predictions array\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and Prepare Data (DO NOT CHANGE)\n",
        "dir_path = './'\n",
        "try:\n",
        "    train_df = load_pubmed_rct_file(os.path.join(dir_path, 'train.txt'))\n",
        "    dev_df   = load_pubmed_rct_file(os.path.join(dir_path, 'dev.txt'))\n",
        "    test_df  = load_pubmed_rct_file(os.path.join(dir_path, 'test.txt'))\n",
        "\n",
        "    train_df = pd.DataFrame({'label': ['BACKGROUND'], 'sentence': ['placeholder']})\n",
        "    dev_df   = pd.DataFrame({'label': ['BACKGROUND'], 'sentence': ['placeholder']})\n",
        "    test_df  = pd.DataFrame({'label': ['BACKGROUND'], 'sentence': ['placeholder']})\n",
        "\n",
        "\n",
        "    print(f\"Train samples: {len(train_df)}\")\n",
        "    print(f\"Dev   samples: {len(dev_df)}\")\n",
        "    print(f\"Test  samples: {len(test_df)}\")\n",
        "\n",
        "    X_train, y_train = train_df['sentence'], train_df['label']\n",
        "    X_dev,   y_dev   = dev_df['sentence'],   dev_df['label']\n",
        "    X_test,  y_test  = test_df['sentence'],  test_df['label']\n",
        "    target_names = sorted(y_train.unique())\n",
        "    print(f\"Classes: {target_names}\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: Dataset file not found. Please ensure the files are uploaded.\")\n",
        "    X_train, y_train = pd.Series([]), pd.Series([])\n",
        "    X_test, y_test = pd.Series([]), pd.Series([])\n",
        "    target_names = []"
      ],
      "metadata": {
        "id": "JJlibd_d9U9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Extraction and Custom Model Training\n",
        "if X_train is not None and len(X_train) > 0:\n",
        "\n",
        "    # Initialize and fit the CountVectorizer for count-based features\n",
        "    count_vectorizer = CountVectorizer(\n",
        "        lowercase=True,\n",
        "        strip_accents='unicode',\n",
        "        stop_words='english',\n",
        "        # // TODO: Set appropriate ngram_range\n",
        "        ngram_range=None,\n",
        "        # // TODO: Set appropriate min_df\n",
        "        min_df=None\n",
        "    )\n",
        "\n",
        "    print(\"Fitting Count Vectorizer and transforming training data...\")\n",
        "    # // TODO: Fit the vectorizer on X_train and transform\n",
        "    X_train_counts = None\n",
        "    if X_train_counts is not None:\n",
        "        print(f\"Vocabulary size: {X_train_counts.shape[1]}\")\n",
        "\n",
        "    print(\"Transforming test data...\")\n",
        "    # // TODO: Transform X_test using the fitted vectorizer\n",
        "    X_test_counts = None\n",
        "\n",
        "\n",
        "    # Train Custom Naive Bayes Classifier\n",
        "    print(\"\\nTraining the Custom Naive Bayes Classifier (from scratch)...\")\n",
        "\n",
        "    # // TODO: Initialize the custom NaiveBayesClassifier\n",
        "    nb_model = None\n",
        "\n",
        "    # // TODO: Fit the model using X_train_counts and y_train\n",
        "    # nb_model.fit(...)\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping feature extraction and training: Training data is empty or not loaded.\")"
      ],
      "metadata": {
        "id": "zko4lfcC9k7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict and evaluate on test set\n",
        "print(\"\\n=== Test Set Evaluation (Custom Count-Based Naive Bayes) ===\")\n",
        "\n",
        "# // TODO: Predict y_test_pred using X_test_counts\n",
        "y_test_pred = None\n",
        "\n",
        "if y_test_pred is not None:\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
        "    print(classification_report(y_test, y_test_pred, target_names=target_names))\n",
        "    test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "    print(f\"Macro-averaged F1 score: {test_f1:.4f}\")\n",
        "else:\n",
        "    print(\"Prediction step failed or incomplete.\")\n"
      ],
      "metadata": {
        "id": "Ef-tgnFD9_86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix on test set\n",
        "    # // TODO: Use the confusion_matrix, matplotlib, and seaborn libraries to generate\n",
        "    # a visual confusion matrix (heatmap) for the predicted results.\n",
        "    # if y_test_pred is not None:\n",
        "    #     cm = confusion_matrix(...)\n",
        "    #     plt.figure(...)\n",
        "    #     sns.heatmap(...)\n",
        "    #     plt.show()"
      ],
      "metadata": {
        "id": "CouhQ1RM-Z_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part B**\n",
        "TF-IDF score based Classifier"
      ],
      "metadata": {
        "id": "AVHIiaD7Bhna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    f1_score\n",
        ")\n",
        "\n",
        "# =======================================================\n",
        "# TODO: Implement the following steps:\n",
        "# 1. Define the initial `pipeline` combining TfidfVectorizer and MultinomialNB with default parameters.\n",
        "# 2. Train the initial pipeline on the training data (X_train, y_train).\n",
        "# 3. Predict and evaluate the performance of the initial model on the test data (X_test, y_test).\n",
        "# 4. Define the `param_grid` for hyperparameter tuning.\n",
        "# 5. Initialize `GridSearchCV` using the pipeline, parameter grid, and appropriate cross-validation settings.\n",
        "# 6. Fit the Grid Search object using the development data (X_dev, y_dev).\n",
        "# 7. Print the `best_params_` and `best_score_` found by the grid search.\n",
        "# =======================================================\n",
        "\n",
        "\n",
        "# // TODO: Define a Pipeline named 'pipeline' using TfidfVectorizer and MultinomialNB.\n",
        "# Use standard initial parameters\n",
        "pipeline = None\n",
        "\n",
        "# // TODO: Train the initial pipeline on the training set\n",
        "print(\"Training initial Naive Bayes pipeline...\")\n",
        "print(\"Training complete.\")\n",
        "\n",
        "\n",
        "# Predict and evaluate on test set\n",
        "# // TODO: Predict y_test_pred and calculate metrics\n",
        "print(\"\\n=== Test Set Evaluation (Initial Sklearn Model) ===\")\n",
        "y_test_pred = None\n",
        "if y_test_pred is not None:\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
        "    print(classification_report(y_test, y_test_pred, target_names=target_names))\n",
        "    print(f\"Macro-averaged F1 score: {f1_score(y_test, y_test_pred, average='macro'):.4f}\")\n",
        "    pass\n",
        "else:\n",
        "    print(\"Initial model evaluation skipped: Predictions not available.\")\n",
        "\n",
        "\n",
        "# Hyperparameter Tuning using GridSearchCV\n",
        "\n",
        "# // TODO: Define the parameter grid 'param_grid' to tune both TF-IDF and NB parameters.\n",
        "param_grid = {\n",
        "    # 'tfidf__ngram_range': [...],\n",
        "    # 'nb__alpha': [...]\n",
        "}\n",
        "\n",
        "# // TODO: Initialize GridSearchCV using the pipeline and param_grid.\n",
        "# Ensure cv=3 and scoring='f1_macro' are used.\n",
        "grid = None\n",
        "\n",
        "print(\"\\nStarting Hyperparameter Tuning on Development Set...\")\n",
        "# // TODO: Fit the GridSearchCV object using the development data.\n",
        "print(\"Grid search complete.\")\n",
        "\n",
        "\n",
        "if grid is not None and hasattr(grid, 'best_params_'):\n",
        "    # // TODO: Print the best parameters and the corresponding best cross-validation score.\n",
        "    pass\n",
        "else:\n",
        "    print(\"Hyperparameter tuning skipped: Grid Search object not initialized or fitted.\")\n"
      ],
      "metadata": {
        "id": "vbMuNX28BhPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part C**\n",
        "Bayes Optimal Classifier"
      ],
      "metadata": {
        "id": "nXeRmXWvGq3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part C Draft"
      ],
      "metadata": {
        "id": "OI3z1Thxz0Rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split # Used for P(h|D) calculation\n",
        "\n",
        "# =======================================================\n",
        "# TODO: Implement the following steps:\n",
        "# 1. Train all five hypotheses on the sampled training data.\n",
        "# 2. Compute the Posterior Weights P(h_i | D) using a validation split.\n",
        "# 3. Fit the VotingClassifier using the sampled training data.\n",
        "# 4. Make final predictions and evaluate the BOC performance on the test data.\n",
        "# =======================================================\n",
        "\n",
        "# Dynamic Data Sampling (DO NOT CHANGE)\n",
        "BASE_SAMPLE_SIZE = 10000\n",
        "\n",
        "# Prompt the user for their full SRN\n",
        "FULL_SRN = input(\"Please enter your full SRN (e.g., PES1UG22CS345): \")\n",
        "\n",
        "try:\n",
        "    # Extract the last three characters and convert to integer\n",
        "    if len(FULL_SRN) >= 3:\n",
        "        srn_suffix_str = FULL_SRN[-3:]\n",
        "        srn_value = int(srn_suffix_str)\n",
        "    else:\n",
        "        # Fallback if input is too short\n",
        "        raise ValueError(\"SRN too short.\")\n",
        "except (ValueError, IndexError, TypeError):\n",
        "    # Fallback if SRN is not entered or format is incorrect\n",
        "    print(\"WARNING: SRN input failed or format is incorrect. Using 10000.\")\n",
        "    srn_value = 0\n",
        "\n",
        "# Calculate the final sample size: 10000 + last three SRN digits\n",
        "SAMPLE_SIZE = BASE_SAMPLE_SIZE + srn_value\n",
        "\n",
        "print(f\"Using dynamic sample size: {SAMPLE_SIZE}\")\n",
        "\n",
        "# Placeholder initialization in case data wasn't loaded in the environment\n",
        "if 'X_train' not in locals() or len(X_train) == 0:\n",
        "    print(\"Warning: Training data not found. Using small placeholder data.\")\n",
        "    X_train = pd.Series([\"sample text one\"] * 11000)\n",
        "    y_train = pd.Series([\"BACKGROUND\"] * 5000 + [\"METHODS\"] * 6000)\n",
        "    X_test = pd.Series([\"test text one\", \"test text two\"])\n",
        "    y_test = pd.Series([\"BACKGROUND\", \"METHODS\"])\n",
        "    target_names = [\"BACKGROUND\", \"CONCLUSIONS\", \"METHODS\", \"OBJECTIVE\", \"RESULTS\"]\n",
        "\n",
        "effective_sample_size = min(SAMPLE_SIZE, len(X_train))\n",
        "X_train_sampled = X_train[:effective_sample_size]\n",
        "y_train_sampled = y_train[:effective_sample_size]\n",
        "print(f\"Actual sampled training set size used: {effective_sample_size}\")\n",
        "\n",
        "\n",
        "# Base TF-IDF parameters (DO NOT CHANGE)\n",
        "tfidf_params = {\n",
        "    'lowercase': True,\n",
        "    'strip_accents': 'unicode',\n",
        "    'stop_words': 'english',\n",
        "    'ngram_range': (1, 1), # Using unigrams only to keep feature space small for diverse models\n",
        "    'min_df': 5\n",
        "}\n",
        "\n",
        "# Define the five diverse hypotheses/pipelines (DO NOT CHANGE)\n",
        "\n",
        "# H1: Multinomial Naive Bayes\n",
        "h1_nb = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(**tfidf_params)),\n",
        "    ('clf', MultinomialNB(alpha=1.0, fit_prior=False))\n",
        "])\n",
        "\n",
        "# H2: Logistic Regression\n",
        "h2_lr = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(**tfidf_params)),\n",
        "    ('clf', LogisticRegression(solver='liblinear', multi_class='auto', max_iter=1000, random_state=42))\n",
        "])\n",
        "\n",
        "# H3: Random Forest Classifier\n",
        "h3_rf = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(**tfidf_params)),\n",
        "    ('clf', CalibratedClassifierCV(\n",
        "        RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42, n_jobs=-1), cv=3, method='isotonic'\n",
        "    ))\n",
        "])\n",
        "\n",
        "# H4: Decision Tree Classifier\n",
        "h4_dt = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(**tfidf_params)),\n",
        "    ('clf', CalibratedClassifierCV(\n",
        "        DecisionTreeClassifier(max_depth=10, random_state=42), cv=3, method='isotonic'\n",
        "    ))\n",
        "])\n",
        "\n",
        "# H5: K-Nearest Neighbors\n",
        "h5_knn = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(**tfidf_params)),\n",
        "    ('clf', CalibratedClassifierCV(\n",
        "        KNeighborsClassifier(n_neighbors=5, n_jobs=-1), cv=3, method='isotonic'\n",
        "    ))\n",
        "])\n",
        "\n",
        "hypotheses = [h1_nb, h2_lr, h3_rf, h4_dt, h5_knn]\n",
        "hypothesis_names = ['NaiveBayes', 'LogisticRegression', 'RandomForest', 'DecisionTree', 'KNN']\n",
        "\n",
        "\n",
        "# Training and BOC Implementation (STUDENT TASK)\n",
        "\n",
        "# // TODO: Train all five hypotheses on X_train_sampled and y_train_sampled using a for loop.\n",
        "print(\"\\nTraining all base models...\")\n",
        "\n",
        "print(\"All base models trained.\")\n",
        "\n",
        "\n",
        "# // TODO: Implement the Posterior Weight Calculation (P(h_i | D)).\n",
        "# This requires splitting X_train_sampled into a small train_sub/val_sub set\n",
        "# and calculating the validation log-likelihood for each model. Normalize these to get posterior_weights.\n",
        "posterior_weights = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
        "\n",
        "\n",
        "# Implement and Evaluate the Bayes Optimal Classifier\n",
        "estimators = list(zip(hypothesis_names, hypotheses))\n",
        "\n",
        "# BOC is approximated using soft voting with posterior weights\n",
        "boc_soft_voter = VotingClassifier(\n",
        "    estimators=estimators,\n",
        "    voting='soft',\n",
        "    weights=posterior_weights,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"\\nFitting the VotingClassifier (BOC approximation)...\")\n",
        "# // TODO: Fit the VotingClassifier using the full sampled training data (X_train_sampled, y_train_sampled)\n",
        "print(\"Fitting complete.\")\n",
        "\n",
        "\n",
        "# Make the final BOC prediction on the test set\n",
        "print(\"\\nPredicting on test set...\")\n",
        "# // TODO: Predict y_pred using X_test, and then calculate and visualize evaluation metrics.\n",
        "y_pred = None\n",
        "\n",
        "\n",
        "# Final Evaluation (STUDENT TASK)\n",
        "print(\"\\n=== Final Evaluation: Bayes Optimal Classifier (Soft Voting) ===\")\n",
        "\n",
        "if y_pred is not None:\n",
        "    # Example calculations:\n",
        "    # // TODO: Generate and visualize the Confusion Matrix (heatmap) for the BOC predictions.\n",
        "    pass\n",
        "else:\n",
        "    print(\"Evaluation skipped: Predictions not generated.\")"
      ],
      "metadata": {
        "id": "qQn3hASQz2K3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}